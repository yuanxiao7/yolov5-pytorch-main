7月30日

## 什么是transformer？

- **Transformer** 就是一个 **基于多头注意力机制的模型**，把注意力机制理解了，这个其实也就很简单了，之前已经详细介绍了注意力机制，在此就细说。 （之前在论文了写的关于Transformer Encoder模型）Transformer Encoder模型的输入是一句话的字嵌入表示和其对应的位置编码信息，模型的核心层是一个多头注意力机制。 注意力机制最初应用在图像特征提取任务上，比如人在观察一幅图像时，并不会把图像中每一个部分都观察到，而是会把注意力放在重要的部分，后来研究人员把注意力机制应用到了\*NLP\*任务中，并取得了很好的效果。 多头注意力机制就是使用多个注意力机制进行单独计算，以获取更多层面的语义信息，然后将各个注意力机制获取的结果进行拼接组合，得到最终的结果。



### encoder——decoder

- encoder--decoder是模拟人类认知的一个过程，

- encoder记忆理解信息，并提炼信息（暂且认为特征提取吧），通常会形成一个低秩的向量（相对于输入而言）

- decoder会议与运用信息，再将低秩的向量的加工后的信息提取出来，这时也可以混合其他信息，解码成需要的形式。

- 例如 基于encoder--decoder的机器翻译，就是说让机器先用一种语言的方式理解这句话，然后再将机器的理解用另一种语言翻译出来。在decoder句子生成的时候还会有些像语言模型的信息加入确保输出的句子像话。